**Contents**
1.   [Introduction](#sec1)
2.   [About The Project](#sec2)
3.   [Problem Definition: Composite Laminates](#sec3)
4.   [Design](#sec4)
5.   [Concurrency](#sec5)

## 1. About The Project <a id='sec1'></a>

This project is a part of the repositories to illustrate my software engineering experience.
This repository especially demonstrates my skills and capabilities in concurrency and GPU parallelism.

In the first section, I will start by describing the problem without diving into too much detail.
Then, I will introduce a brief comparison about deterministic and heuristic/genetic solutions.
Finally, I will discuss the issues about performance and concurrency.

**Nomenclature**
- **GA:** Genetic Algorithm
- **CLT:** Classical Lamination Theory
- **ABD:** ABD matrix, stiffness matrix containing A, B, and D matrices
- **A matrix:** The extensional stiffness matrix
- **B matrix:** The coupling matrix between A and D
- **D matrix:** The rotational stiffness matrix
- **NP:** The stacking sequence ply count
- **NE:** The size of elite population
- **NC:** The size of crossover population where NC = NE^2

## 2. Problem Definition: Composite Laminates <a id='sec2'></a>

Composite laminates are used widely in the industry due to their high stiffness capabilities.
The stiffness of a composite laminate is strictly based on how the plies are arranged.
For example, a laminate which is mainly formed by 0-degree plies would withstand high loading in the first direction
while failing in the second direction easily due to material failure or instability.

The sequence of ply angles is called a *layup sequence* or *stacking sequence*.
Designers need to set up the stacking sequence for every new problem or optimize for combined loading.
Determination of the stacking sequence is the most critical process in the design of a laminate.
It is very easy if the number of plies is not large (e.g., 10 plies).
Even a brute force algorithm would solve such a problem in seconds by traversing all possible solutions.
However, the brute force solution requires large power and infinite resources if the ply count is high.
For example, let's consider a laminate with 100 plies and assume only 4 angles are allowed: 0, +45, -45, and 90.
This problem would result in 4^100 candidate solutions.

The grade of a candidate solution is determined by the strength analysis.
The strength analysis can either be as complicated as buckling stiffness determination or as simple as ABD calculation.
I will limit this measurement by ABD matrix to be simple.
For example, A11 simulates the extensional stiffness in the 1st direction,
while D22 simulates the rotational/bending stiffness in the 2nd direction.
The details can be found in any source about CLT.

**Note**\
Instead of the ply angles, I will use an indexing system for the ply angles in order to improve memory and performance efficiency:
- 0: 0-degree plies
- 1: 90-degree plies
- 2: +45-degree plies
- 3: -45-degree plies

Besides, composite theory uses angle brackets and backslashes in order to demonstrate stacking sequences:
- Example: [0/45/90/-45/-45/90/45/0] == [0/45/90/-45]s

Refer to any composite source for more details about the issue.

## 3. The Genetic Algorithm <a id='sec3'></a>

A brief analysis of the problem shows that the GA is a good candidate for the solution.
In the **base case**, the GAs would easily eliminate the unreasonable sequences and direct the available resources for the ones around the local and global extremities.
However, the composite theory contains many rules (i.e. the stacking rules) defining the limitations and exceptions to that base solution:
- The sequence must be **symmetric** (i.e. [a1/a2/a3/.../an/an/.../a3/a2/a1])
- The number of +45 plies and -45 plies must be the same (i.e. the **balance**)
- An angle cannot be repeated more than 4 times (e.g. [.../0/0/0/0/...] not acceptable)
- Ply angle change must be less than 90 (e.g. [.../0/90/...] not acceptable)
- The OML plies must be +45 or -45 ([-45/.../.../-45] or [+45/.../.../+45])
- Each angle shall present at least 10 percent
- etc.

Some of the stacking rules comes from the fundamental theory (e.g. CLT) while some other comes from more complex areas (e.g. composite fracture).
Some of them simulates the pure theory while some others are empirical.
The stacking rules bring too much determinism into the problem weakening the efficiency of the heuristic approaches.
**In other words, the stacking rules pollute the fitness measurement of the GAs.**
Let's name the genes not obeying the stacking rules as **failed genes**.
In case of the GAs, the failed genes would be generated by the crossover and/or mutation operations and eliminated after the fitness measurement process.
On the other side, a deterministic algorithm would not allow the failed genes to be generated at all.
Hence, the efficiency of the GAs is questionable due to the stacking rules.
For example, for a stacking with 8 plies we have 4^8 = 65536 solutions in the base case.
However, considering the stacking rules, **there exist only 4 solutions**:
- Solution 1: [+45/0/-45/90]s
- Solution 2: [+45/90/-45/0]s
- Solution 3: [-45/0/+45/90]s
- Solution 4: [-45/90/+45/0]s

I will leave this discussion at this point as I am not a GA specialist.
I will neglect the stacking rules except for the critical two in order to remain in a reasonable situation with the GA:
- The first rule: Define the stacking as a symmetric half sequence.
- The second rule: The balance is crucial and can be measured by A16 stiffness term.

Although I know the existence of the techniques to improve the performance of a GA, I don't have a competitive experience in this field.
Hence, **the GA in this project is a primitive version** with the following steps:
1. **Generation:** Generate a new population as the initial elite population
2. **Crossover:** Perform crossover on the existing elite generation
3. **Mutation:** Mutate the crossovers
4. **Measurement:** Measure the fitness rates for the mutations
5. **Selection:** Obtain the new elites from the mutations based on the fitness rates
6. **Inspection:** If local maxima failure is detected, go to Step 1, otherwise go to Step 2

One of the main problems of GAs is to escape local maxima/minima.
Various techniques/strategies exist to solve the problem such as:
- Mathematical tools (e.g. gradient inspection)
- Population strategies (e.g. multi-population approach)

I used the simplest one which is the Step 6 in the above flowchart:
- Restart with a new generation.

## 4. Design <a id='sec4'></a>

A GA requires answers to the following questions:
- How to define a gene?
- How to define the populations/generations?
- How to perform the fitness measurement?
- How to perform the crossover and mutation?
- Concurrency?

An initial analysis of the problem shows that a stacking sequence is a combination of ply angles which can have a value between 0 and 3.
The crossover, mutation, and selection are all simple operations containing integral data.
The problem does not define or require relationships between the data which may require pointer-based data structures.
Hence, simple/fundamental data types and containers are sufficient.

The first review of the problem and the solution can be improved.
**Approach 1** below discusses the first solution while **Approach 2** describes an improvement.
Later in this section, I will discuss about the concurrency which will affect the whole design deeply.

**Approach 1**\
The traditional definition for a stacking is an array of the angles: `unsigned char[N]`.
The unsigned char data type is more than enough to store the 4 angle indices: 0, 1, 2, and 3.
This definition is very efficient considering the 3rd question, as the ABD matrix contains the summation of the stiffness terms calculated per ply.
In order for the performance, the ABD value for each ply corresponding to the 4 angles can be **pre-calculated and cached** before starting the GA.
This cache can be defined by two multi-dimensional arrays:
- `A_cache[angle id][stiffness id]`
- `D_cache[angle id][ply id][stiffness id]`

For example:
- `A_cache[0][0]`: A11 for 0-degree plies
- `A_cache[3][0]`: A11 for 90-degree plies
- `A_cache[0][1]`: A12 for 0-degree plies
- `D_cache[0][0][0]`: D11 if the 0th ply is 0-degree
- `D_cache[0][0][1]`: D11 if the 0th ply is +45-degree
- `D_cache[0][1][0]`: D11 if the 1st ply is 0-degree
- `D_cache[1][0][0]`: D12 if the 0th ply is 0-degree

Then, the GA fitness determination is reduced to a simple summation process.

Approach 1 would implement the crossover operation with an array copy operation.
The 1st crossover copies the 1st half from the 1st elite and the 2nd half from the 2nd elite.
The 2nd crossover reverses the copy order.
Following 4-ply-stackings demonstrate the crossover operation for this approach:
- Elite 1: `[0, 0, 0, 0]`
- Elite 2: `[3, 3, 3, 3]`
- Crossover 1: `[0, 0, 3, 3]`
- Crossover 2: `[3, 3, 0, 0]`

Although this copy operation is a bitwise copy and fast,
the crossover operation is still the hot point in the program flow,
as the 6 bits of the unsigned char do not contain data but need to be copied for each ply.

**Approach 2**\
The crossover operation can be lifted by defining **the whole stacking** with an **integral data type**.
For example, `uint64_t` can store up to 32 plies.
By this conversion, the crossover operation can be processed with the core CPU operations such as bit masking and shifting with almost no cost.
Following 4-ply-stackings demonstrate the crossover operation for this approach:
- Elite 1: `[0, 0, 0, 0]`: `00000000` (digital) or `0x0` (hexadecimal)
- Elite 2: `[3, 3, 3, 3]`: `11111111` (digital) or `0xFF` (hexadecimal)
- Crossover 1: `[0, 0, 3, 3]`: `00001111` (digital) or `0xF` (hexadecimal)
- Crossover 2: `[3, 3, 0, 0]`: `11110000` (digital) or `0xF0` (hexadecimal)

The downside of Approach 2 is the fitness calculation (i.e. ABD) which requires extracting the 2-bit angles from the integral data:
- `0xFF` (hexadecimal): `[3, 3, 3, 3]`

Hence, Approach 2 improves to crossover a lot while complicating the fitness calculation.
The balance between the two implications spoils down toward Approach 2 as the ply count increases.
My trade-offs show that for a stacking with 32 plies, Approach 2 results with an approximately 10 times better performance for the whole GA.
Hence, I will follow Approach 2.

This project implements the GA for a 32-ply-stacking.
Approach 2 is followed defining the stacking sequence by `uint64_t` data type.
The algorithm could have been implemented generically for all stacking sizes by defining the stacking by `uint64_t[N]` where N could be determined statically.
This would have added little complexity to the solution.
However, I did not implement the generic solution **for simplicity** and kept the stacking ply count constant (i.e. `#define PLY_COUNT 32`).

The multi-dimensional cache arrays (**A_cache** and **D_cache**) contain data per ply.
The data in the two arrays are summed up in Step 4 to achieve the ABD matrix.
The summation requires N operation where N is the stacking ply count.
However, this N operations need to be performed for all genes.
Another performance improvement would be achieved by defining packs of plies.
Let's define **the stacking pack** as the pack of P plies.
A and D arrays can be calculated per stacking pack which will reduce the workload of the GA P times:
- `A_cache[angle id][stiffness id]`
- `D_cache[stacking pack id][stiffness id]`

Similar to the stacking, a stacking pack can also be defined by an integral data type.
The optimum value for P is 4.
The optimum data type is `uint16_t` which can simulate up to 65536 packs, for which D_cache stores 65536 x 3 = 196608 values, where 3 is the number of stiffness terms.
The stacking pack id would store two data:
- 8 bits: 4-ply-sequence
- 8 bits: A position flag which marks the position of the pack within the stacking.

The position flag (8 bits) can rise up to 256.
Hence, for this solution, the ply count for a stacking is limited by: 256 * 4 = 1024.

The stacking pack improvement adds lots of complexity to the algorithm.
**Hence, I will not implement this improvement for simplicity.**

**Concurrency**\
The next section covers all the related issues about concurrency, which suggests two solutions based on the two fundamental approaches:
1. Data parallelism
2. Task parallelism

The details about the two solutions will be explained in the next section.
It's important to note here that the two would require different data structures:
- Data parallelism: Contiguous arrays (e.g. `elites_array[]`, `crossovers_array[]`)
- Task parallelism: Thread-safe queues (e.g. `elites_queue`, `crossovers_queue`)

## 5. Concurrency <a id='sec5'></a>

Let's recall the flowchart of the genetic solution:
1. Generation: Generate a new population as the initial elite population
2. Crossover: Perform crossover on the existing elite generation
3. Mutation: Mutate the crossovers
4. Measurement: Measure the fitness rates for the mutations
5. Selection: Obtain the new elites from the mutations based on the fitness rates
6. Inspection: If maxima is detected goto Step 1, otherwise goto Step 2

We can have two approaches in case of the concurrency:
- Data parallelism: Synchronous execution
- Task parallelism: Asynchronous execution with producer-consumer strategy

**Data Parallelism**\
The data parallelism corresponds to a **population-wise** strategy.
In other words, the algorithm is designed
considering the populations (e.g. elites or crossovers).
Each process operates on the corresponding population parallely instead of the individual genes.

The synchronization is ensured by executing the flowchart sequentially.
Each step spawns a number of parallel threads which process the genes from the array of the previous step and update the genes from the array of the next step.
For example, each thread spawned in Step 3 reads a number of crossover genes from the crossover array and updates the mutation array with the mutations of those crossover genes.
Executing each step sequentially, the solution **does not need any synchronization primitive (e.g. locks or atomics)**.
The whole parallel capacity of the platform can be utilized efficiently.

The genes/data have a constant size and is stored in constant size arrays.
Additionally, the operations in the flowchart have no branches so that the same pattern is followed for all data.
Having **the same fundamental operations on the integral data**, we can say that all threads executing a function would yield in **approximately the same time**.
**So, the resources are used effectively without leading inert processors/threads.**

At this point, it's worth to discuss about the time complexity analysis.
**Assuming the existence of sufficient processors for parallel execution**, below are the time complexities for each step of the GA:
1. Generation: O(1)
2. Crossover: O(1)
3. Mutation: O(1)
4. Measurement: O(NP)
5. Selection (i.e. sort): O(NC) where NC = NE ^ 2
6. Inspection (i.e. max element): O(logNC) ~ O(NE) where NC = NE ^ 2

The complexities for the first 3 steps are obvious.
The Measurement process composes the calculation of the ABD matrix, which is reduced to the ply-wise summation of the cache arrays: A_cache and D_cache.
The complexity O(NP) can further be improved as O(1) by increasing the parallelism if more processor support is available.
See the documentations of `max_element_no_recursion__h` and `sort_indexs_by_max_element_no_recursion__h` functions for the complexities of the selection and inspection.

The obvious conclusion the above complexities point is that the selection step (Step 5) dominates the flow.
In other words, the complexity of one cycle of the GA is approximately O(NC) ~ O(NEÂ²), which is **parabolic** in terms of the elite population size.
This is the main problem with data parallelism.
The solution is obvious: Improve the selection (i.e. sort) algorithm.
See the documentations of `sort_indexs_by_max_element_no_recursion__h` function for this issue.

**Task Parallelism**\
In contrast to the data parallelism, the task parallelism corresponds to a **gene-wise** strategy.
In other words, the algorithm is designed considering the genes (e.g. elite or crossover).
Each process operates on the individual genes (or pairs) instead of the populations.

The thread-safe containers (i.e. thread_safe_queue) store the populations/generations.
Each step **consumes** genes from a queue and **produces** new genes for another one.
For example, a thread corresponding to Step 2 would consume two elite genes (if exist) from the elite population queue,
produce a crossover gene and push it into the crossover population queue.
Asynchronously, another thread corresponding to Step 3 would consume a crossover gene (if exists) from the crossover population queue,
mutate the gene and push it into the mutation population queue.

The producer-consumer strategy is a reader and writer combination which requires a **synchronization** between the threads.
This can be accomplished by lock-based (i.e. mutex) or lock-free (i.e. atomic) containers.
However, **either of the two provides wait-free execution**.
As the complexities show, the genetic functions execute in a short time such that thread synchronization would affect the total runtime significantly.
**This is the 1st problem with the solution based on task parallelism.**

**The 2nd problem** is about the selection step.
The selection process is actually a sort algorithm, which by its nature,
requires that the related data (i.e. the mutations and the unsorted fitness rates) should be prepared prior to the execution.
**In other words, the sort operation cannot be performed asynchronously, and the previous 4 steps are serialized with this operation.**

A solution to this problem is to modify the selection logic.
In case of data parallelism, the selection operates on the mutation population
by sorting the fitness rates in a descending order and taking the first NE elements from the mutation population based on this sorted fitness rate.
In case of task parallelism, the selection would operate like a maximum algorithm:
- Cache the current maximum fitness rate,
- Select a mutation gene if its fitness rate is higher than the current maximum,
- Update the maximum fitness rate.

This algorithm can be executed asynchronously with the other steps of the GA.
However, this solution would not generate diversity among the elites as *many good genes* would be elected due to this maximum strategy.
The *only the best genes* strategy can be relaxed by replacing the maximum requirement with some percent of the maximum (e.g. 95%).
This approach performs well out of the local/global maxima.
Around the critical regions, on the other hand, most of the genes (maybe all!) would fail to pass the limitation (i.e. 95%)
which means that the selection fails to feed the elite population container.
In such cases (genes less than 95%), the selection would generate random genes in order to keep the processors hot working on the elite population container.

Another problem with task parallelism is about the crossover operation.
In case of data parallelism, an elite gene can be combined with all the other elites
which allows us to have **polygamy** in the crossover operation.
**This will increase the genetic diversity among the crossover population.**
However, in case of task parallelism, two elite genes are consumed asynchronously before other elites arrive in the elite population container.

In summary, the task parallelism has the following problems:
1. The shared data needs to be synchronized (i.e. mutex or atomic).
2. The population is dominated by random genes around the local/global maxima.
3. The elite genes in the population may belong to different local/global maxima.
4. The crossover operation cannot be implemented to support polygamy.

As stated before, I am not an expert on GAs.
However, in my opinion, the task parallelism is not appropriate in case of the GAs
unless solutions to the last three of the problems above can be found.

**I selected the data parallelism in my solution.**
